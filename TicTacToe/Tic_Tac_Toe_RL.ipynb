{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning Agent to play **Tic Tac Toe** Game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Importing all the required libraries.\n",
    "import numpy as np\n",
    "import pickle\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dimensions of the board.\n",
    "BoardRows = 3\n",
    "BoardCols = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Board State\n",
    "- Let us denote the 2 players as **p1** and **p2**.\n",
    "- The two players use **+1**, **-1** which stand for **X** and **O** respectively.\n",
    "- Vacant cells are represented as **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State:\n",
    "    \n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BoardRows, BoardCols))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.StateHashValue = None\n",
    "        \n",
    "        # We are setting the player `p1` to play first.\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    \n",
    "    # Making a unique hash value for the current state of board. (easy to store in State-Value dict. later)\n",
    "    def computeHash(self):\n",
    "        self.StateHashValue = str(self.board.reshape(BoardCols*BoardRows))\n",
    "        return self.StateHashValue\n",
    "    \n",
    "    \n",
    "    # Returns the list of empty cells.\n",
    "    def emptyCells(self):\n",
    "        positions = []\n",
    "        for i in range(BoardRows):\n",
    "            for j in range(BoardCols):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j)) \n",
    "        return positions\n",
    "    \n",
    "    \n",
    "    # It finds out the winner of the game if it ends.\n",
    "    # Returns +1    if Player 1 (p1) is the winner\n",
    "    # Returns -1    if Player 2 (p2) is the winner\n",
    "    # Returns 0     if game results in draw.\n",
    "    # Returns None  if game did not end.\n",
    "    def findWinner(self):\n",
    "        \n",
    "        # Checking for `row` wise matching of each player\n",
    "        for i in range(BoardRows):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "            \n",
    "        # Checking for `column` wise matching of each player\n",
    "        for i in range(BoardCols):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "            \n",
    "         # Checking for `diagonal` wise matching of each player\n",
    "        diagSum1 = sum([self.board[i, i] for i in range(BoardCols)])\n",
    "        diagSum2 = sum([self.board[i, BoardCols-i-1] for i in range(BoardCols)])\n",
    "        \n",
    "        diagSum = max(diagSum1, diagSum2)\n",
    "        \n",
    "        if diagSum == 3:\n",
    "            self.isEnd = True\n",
    "            return 1\n",
    "        if diagSum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        # Checking if the game results in tie ( no available positions )\n",
    "        if len(self.emptyCells()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "    \n",
    "        # If none of the above cases occur, It means the game is not going to end currently.\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # It fills the current symbol in the board at given position & toggles player\n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "    \n",
    "    \n",
    "    # When the game ends, we backpropagate the reward for the players accordingly. \n",
    "    def giveReward(self):\n",
    "        result = self.findWinner()\n",
    "        \n",
    "        # Player 1 won the game\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(0)\n",
    "            \n",
    "        # Player 2 won the game\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(1)\n",
    "            \n",
    "        # Game resulted in draw.\n",
    "        # Note: We are considering that DRAW is a also a bad end. \n",
    "        # Since player 1 started the game, we gave him less reward than player 2.\n",
    "        else:\n",
    "            self.p1.feedReward(0.2)\n",
    "            self.p2.feedReward(0.5)\n",
    "            \n",
    "    \n",
    "    # Completely resets the state of the board.\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BoardRows, BoardCols))\n",
    "        self.StateHashValue = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "        \n",
    "    \n",
    "    # Train the RL agent by making two players play among themselves.\n",
    "    def train(self, rounds = 10000):\n",
    "        \n",
    "        print(\"Training...-> RL agents are learning by playing against each other for \", rounds, \" games!!\\n\" )\n",
    "        \n",
    "        for i in range(rounds):\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(\"Currently at Round {}\".format(i))\n",
    "                \n",
    "            while not self.isEnd:\n",
    "                \n",
    "                # Player 1\n",
    "                positions = self.emptyCells()\n",
    "                p1Action = self.p1.pickAction(positions, self.board, self.playerSymbol)\n",
    "                \n",
    "                # Once the player 1 takes an action, update board state & get Hash of the current state\n",
    "                self.updateState(p1Action)\n",
    "                board_hash = self.computeHash()\n",
    "                self.p1.storeState(board_hash)\n",
    "                \n",
    "                # Check board status if it is end\n",
    "                win = self.findWinner()\n",
    "                \n",
    "                # Game ended with p1 either win or draw\n",
    "                if win is not None:\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.emptyCells()\n",
    "                    p2Action = self.p2.pickAction(positions, self.board, self.playerSymbol)\n",
    "                    \n",
    "                     # Once the player 2 takes an action, update board state & get Hash of the current state\n",
    "                    self.updateState(p2Action)\n",
    "                    board_hash = self.computeHash()\n",
    "                    self.p2.storeState(board_hash)\n",
    "                    \n",
    "                    # Check board status if it is end\n",
    "                    win = self.findWinner()\n",
    "                    \n",
    "                    # Game ended with p2 either win or draw\n",
    "                    if win is not None:\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Using the Trained RL agent to play against Human.\n",
    "    def playAgainstHuman(self):\n",
    "        \n",
    "        while not self.isEnd:\n",
    "            \n",
    "            # Player 1\n",
    "            positions = self.emptyCells()\n",
    "            p1Action = self.p1.pickAction(positions, self.board, self.playerSymbol)\n",
    "            \n",
    "            # Once the player 1 takes an action, update board state \n",
    "            self.updateState(p1Action)\n",
    "            self.printBoard()\n",
    "            \n",
    "            # check board status if it is end\n",
    "            win = self.findWinner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie:)\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.emptyCells()\n",
    "                p2Action = self.p2.pickAction(positions)\n",
    "\n",
    "                self.updateState(p2Action)\n",
    "                self.printBoard()\n",
    "                win = self.findWinner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie:)\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    # Prints the board to the console.\n",
    "    # p1 moves stands for X\n",
    "    # p2 moves stands for O\n",
    "    def printBoard(self):\n",
    "        for i in range(0, BoardRows):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BoardCols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    coin = 'X'\n",
    "                if self.board[i, j] == -1:\n",
    "                    coin = 'O'\n",
    "                if self.board[i, j] == 0:\n",
    "                    coin = ' '\n",
    "                out += coin + ' | '\n",
    "            print(out)\n",
    "        print('-------------\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing the RL agent.\n",
    "class Player:\n",
    "    \n",
    "    def __init__(self, name, ExpRate=0.35, learningRate=0.2):\n",
    "        # It records all the positions/states taken by player till the end of the game\n",
    "        self.states = []  \n",
    "        \n",
    "        self.name = name\n",
    "        self.learningRate = learningRate\n",
    "        self.ExpRate = ExpRate\n",
    "        self.decay_gamma = 0.9\n",
    "        \n",
    "         # Dict. to store the Value Function: ( state -> value ) that gets updated at the end of each game\n",
    "        self.states_value = {} \n",
    "    \n",
    "    def computeHash(self, board):\n",
    "        boardHash = str(board.reshape(BoardCols*BoardRows))\n",
    "        return boardHash\n",
    "    \n",
    "     # Takes an action using Epsilon Greedy Policy\n",
    "    def pickAction(self, positions, current_board, symbol):\n",
    "        \n",
    "        # Take a random action\n",
    "        if np.random.uniform(0, 1) <= self.ExpRate:\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        \n",
    "        # Take a greedy action\n",
    "        else:\n",
    "            value_max = -100000\n",
    "            # We hash the next board state and choose the action greedily that returns the maximum value of next state.\n",
    "            for p in positions:\n",
    "                NextBoard = current_board.copy()\n",
    "                NextBoard[p] = symbol\n",
    "                NextBoardHash = self.computeHash(NextBoard)\n",
    "                value = 0 if self.states_value.get(NextBoardHash) is None else self.states_value.get(NextBoardHash)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "                    \n",
    "\n",
    "        return action\n",
    "    \n",
    "    def storeState(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    # At the end of game, backpropagate and updating the Value Function ( State-Value )\n",
    "    # For updating the value estimation of states, we are applying VALUE ITERATION which is as follows:\n",
    "    # V(s_t) =  V(s_t)  + Alpha( V(s_t+1) - V(s_t) )\n",
    "    # V(s_t+1) = 0 + Gamma*Reward (since Immediate reward is 0)\n",
    "    \n",
    "    def feedReward(self, reward):\n",
    "        for state in reversed(self.states):\n",
    "            if self.states_value.get(state) is None:\n",
    "                self.states_value[state] = 0\n",
    "            self.states_value[state] += self.learningRate*(self.decay_gamma*reward - self.states_value[state])\n",
    "            reward = self.states_value[state]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "    \n",
    "    # Saving the trained Policy\n",
    "    def PolicySaver(self):\n",
    "        fileToSave = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fileToSave)\n",
    "        fileToSave.close()\n",
    "    \n",
    "    # Loading the trained Policy\n",
    "    def PolicyLoader(self, file):\n",
    "        fileOpened = open(file,'rb')\n",
    "        self.states_value = pickle.load(fileOpened)\n",
    "        fileOpened.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that allows Human Player to take actions.\n",
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def pickAction(self, positions):\n",
    "        while True:\n",
    "            row, col = [int(x) for x in input(\"Enter position to place your coin - O (row, col) : \").split()] \n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "            else:\n",
    "                print(\"You have selected a Non empty cell. Pick a different cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...-> RL agents are learning by playing against each other for  50000  games!!\n",
      "\n",
      "Currently at Round 0\n",
      "Currently at Round 1000\n",
      "Currently at Round 2000\n",
      "Currently at Round 3000\n",
      "Currently at Round 4000\n",
      "Currently at Round 5000\n",
      "Currently at Round 6000\n",
      "Currently at Round 7000\n",
      "Currently at Round 8000\n",
      "Currently at Round 9000\n",
      "Currently at Round 10000\n",
      "Currently at Round 11000\n",
      "Currently at Round 12000\n",
      "Currently at Round 13000\n",
      "Currently at Round 14000\n",
      "Currently at Round 15000\n",
      "Currently at Round 16000\n",
      "Currently at Round 17000\n",
      "Currently at Round 18000\n",
      "Currently at Round 19000\n",
      "Currently at Round 20000\n",
      "Currently at Round 21000\n",
      "Currently at Round 22000\n",
      "Currently at Round 23000\n",
      "Currently at Round 24000\n",
      "Currently at Round 25000\n",
      "Currently at Round 26000\n",
      "Currently at Round 27000\n",
      "Currently at Round 28000\n",
      "Currently at Round 29000\n",
      "Currently at Round 30000\n",
      "Currently at Round 31000\n",
      "Currently at Round 32000\n",
      "Currently at Round 33000\n",
      "Currently at Round 34000\n",
      "Currently at Round 35000\n",
      "Currently at Round 36000\n",
      "Currently at Round 37000\n",
      "Currently at Round 38000\n",
      "Currently at Round 39000\n",
      "Currently at Round 40000\n",
      "Currently at Round 41000\n",
      "Currently at Round 42000\n",
      "Currently at Round 43000\n",
      "Currently at Round 44000\n",
      "Currently at Round 45000\n",
      "Currently at Round 46000\n",
      "Currently at Round 47000\n",
      "Currently at Round 48000\n",
      "Currently at Round 49000\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"p1\")\n",
    "p2 = Player(\"p2\")\n",
    "state = State(p1, p2)\n",
    "state.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.PolicySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.PolicyLoader(\"policy_p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human vs RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | X |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "Enter position to place your coin - O (row, col) : 0 0\n",
      "-------------\n",
      "| O |   |   | \n",
      "-------------\n",
      "|   | X |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| O | X |   | \n",
      "-------------\n",
      "|   | X |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "\n",
      "Enter position to place your coin - O (row, col) : 0 1\n",
      "You have selected a Non empty cell. Pick a different cell.\n",
      "Enter position to place your coin - O (row, col) : 2 1\n",
      "-------------\n",
      "| O | X |   | \n",
      "-------------\n",
      "|   | X |   | \n",
      "-------------\n",
      "|   | O |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| O | X |   | \n",
      "-------------\n",
      "|   | X | X | \n",
      "-------------\n",
      "|   | O |   | \n",
      "-------------\n",
      "\n",
      "Enter position to place your coin - O (row, col) : 1 0\n",
      "-------------\n",
      "| O | X |   | \n",
      "-------------\n",
      "| O | X | X | \n",
      "-------------\n",
      "|   | O |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| O | X |   | \n",
      "-------------\n",
      "| O | X | X | \n",
      "-------------\n",
      "| X | O |   | \n",
      "-------------\n",
      "\n",
      "Enter position to place your coin - O (row, col) : 0 1\n",
      "You have selected a Non empty cell. Pick a different cell.\n",
      "Enter position to place your coin - O (row, col) : 0 2\n",
      "-------------\n",
      "| O | X | O | \n",
      "-------------\n",
      "| O | X | X | \n",
      "-------------\n",
      "| X | O |   | \n",
      "-------------\n",
      "\n",
      "-------------\n",
      "| O | X | O | \n",
      "-------------\n",
      "| O | X | X | \n",
      "-------------\n",
      "| X | O | X | \n",
      "-------------\n",
      "\n",
      "tie:)\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"computer\")\n",
    "p1.PolicyLoader(\"policy_p1\")\n",
    "p2 = HumanPlayer(\"human\")\n",
    "state = State(p1, p2)\n",
    "state.playAgainstHuman()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.arange(0.1, 1, 0.01)\n",
    "alpha   = np.arange(0.1, 1, 0.01)\n",
    "for i in len(epsilon):\n",
    "    p1 = Player(\"computer\",epsilon,alpha)\n",
    "    p1.PolicyLoader(\"policy_p1\")\n",
    "    p2 = HumanPlayer(\"human\")\n",
    "    state = State(p1, p2)\n",
    "    state.playAgainstHuman()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
