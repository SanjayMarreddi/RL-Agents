{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "360df9a5405d3b6c530f119b1aca4206c69f0c787f7f5a2c11f442b93e5a1a7f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('WindowsApps')"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "FrozenLake_MC_TD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SiHjdsUcR4OZ",
        "jNPVKX2MOyj0",
        "cmOvtVtqOyj6",
        "h1obMRi1Oyj-",
        "RNVqsQ58OykA",
        "YfDEfFmtOykC"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiHjdsUcR4OZ"
      },
      "source": [
        "### Basic Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsVjVmpHOyjk"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "%autosave 5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\RITCDE~1\\AppData\\Local\\Temp/ipykernel_11472/188785255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autosave'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIDmhxl6Oyjp"
      },
      "source": [
        "class State:\n",
        "    def __init__(self, position, is_terminal=False):\n",
        "        self.position = position\n",
        "        self.term = is_terminal\n",
        "        self.r = 0  # reward\n",
        "\n",
        "class Action:\n",
        "\n",
        "    L=0\n",
        "    R=1\n",
        "    U=2\n",
        "    D=3\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.action_space=4\n",
        "        self.observation_space=9\n",
        "        self.rewards_matrix=[[0,0,0],\n",
        "                     [0,1,-10],\n",
        "                     [1,0,10]]\n",
        "        \n",
        "        # self.rewards_matrix -= 0.1\n",
        "    \n",
        "    \n",
        "    def get_start_state(self):\n",
        "        s = State(0)\n",
        "        return s\n",
        "        \n",
        "    def sample(self):\n",
        "        return np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
        "    \n",
        "    \n",
        "    def step(self, s, action):\n",
        "        \n",
        " \n",
        "        r = 0\n",
        "            \n",
        "        # Player is in the 1st cell\n",
        "        if(s.position==0):\n",
        "            \n",
        "            # Choosen action is to move LEFT.\n",
        "            if(action==0):\n",
        "                \n",
        "                # With prob 0.5 each he will move to state 1 or 2.\n",
        "                s.position=np.random.choice([1,2],p=[0.5,0.5])\n",
        "                \n",
        "                # If he moves to state 2, He took 1 step, so in rewards, we subtract 0.1\n",
        "                if(s.position==2):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "              \n",
        "                 # If he moves to state 1, He took 2 steps, so in rewards, we subtract 0.2\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2   \n",
        "            \n",
        "                # Note: Subtracting 0.1 helps in finding shortest path.\n",
        "\n",
        "                s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([1,2],p=[0.5,0.5])\n",
        "                if(s.position==2):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([3,6],p=[0.5,0.5])\n",
        "                if s.position==3:\n",
        "                    r=self.rewards_matrix[1][0] -0.2\n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][0] -0.1\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([3,6],p=[0.5,0.5])\n",
        "                if s.position==3:\n",
        "                    r=self.rewards_matrix[1][0] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][0] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "        elif(s.position==1):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([0,2],p=[0.5,0.5])\n",
        "                if(s.position==2):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "                s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([0,2],p=[0.5,0.5])\n",
        "                if(s.position==2):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([4,7],p=[0.5,0.5])\n",
        "                if s.position==4:\n",
        "                    r=self.rewards_matrix[1][1] -0.2\n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][1] -0.1\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([4,7],p=[0.5,0.5])\n",
        "                if s.position==4:\n",
        "                    r=self.rewards_matrix[1][1] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][1] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "        elif(s.position==2):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([1,0],p=[0.5,0.5])\n",
        "                if(s.position==1):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([1,0],p=[0.5,0.5])\n",
        "                if(s.position==0):\n",
        "                    r=self.rewards_matrix[0][s.position] -0.1\n",
        "                else:\n",
        "                    r=self.rewards_matrix[0][s.position] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([5,8],p=[0.5,0.5])\n",
        "                if s.position==5:\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][2] \n",
        "                s.term=True\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([5,8],p=[0.5,0.5])\n",
        "                if s.position==5:\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][2] \n",
        "                s.term=True\n",
        "                \n",
        "        elif(s.position==3):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([5,4],p=[0.5,0.5])\n",
        "                if(s.position==5):\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                    s.term=True\n",
        "                else:\n",
        "                    r=self.rewards_matrix[1][1] -0.2\n",
        "                    s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([5,4],p=[0.5,0.5])\n",
        "                if(s.position==4):\n",
        "                    r=self.rewards_matrix[1][1] -0.1\n",
        "                    s.term=False\n",
        "                else:\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                    s.term=True\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([0,6],p=[0.5,0.5])\n",
        "                if s.position==6:\n",
        "                    r=self.rewards_matrix[1][0] -0.2\n",
        "                else :\n",
        "                    r=self.rewards_matrix[2][0] -0.1\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([0,6],p=[0.5,0.5])\n",
        "                if s.position==6:\n",
        "                    r=self.rewards_matrix[2][0] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[0][0] -0.2\n",
        "                s.term=False\n",
        "                   \n",
        "        elif(s.position==4):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([5,3],p=[0.5,0.5])\n",
        "                if(s.position==5):\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                    s.term=True\n",
        "                else:\n",
        "                    r=self.rewards_matrix[1][0] -0.2\n",
        "                    s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([5,3],p=[0.5,0.5])\n",
        "                if(s.position==3):\n",
        "                    r=self.rewards_matrix[1][0] -0.1\n",
        "                    s.term=False\n",
        "                else:\n",
        "                    r=self.rewards_matrix[1][2] \n",
        "                    s.term=True\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([1,7],p=[0.5,0.5])\n",
        "                if s.position==7:\n",
        "                    r=self.rewards_matrix[2][1] -0.2\n",
        "                else :\n",
        "                    r=self.rewards_matrix[0][1] -0.1\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([1,7],p=[0.5,0.5])\n",
        "                if s.position==7:\n",
        "                    r=self.rewards_matrix[2][1] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[0][1] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "        elif(s.position==6):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([7,8],p=[0.5,0.5])\n",
        "                if(s.position==8):\n",
        "                    r=self.rewards_matrix[2][2] \n",
        "                    s.term=True\n",
        "                else:\n",
        "                    r=self.rewards_matrix[2][1] -0.2\n",
        "                    s.term=False\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([7,8],p=[0.5,0.5])\n",
        "                if(s.position==7):\n",
        "                    r=self.rewards_matrix[2][1] -0.1\n",
        "                    s.term=False\n",
        "                else:\n",
        "                    r=self.rewards_matrix[2][2] \n",
        "                    s.term=True\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([0,3],p=[0.5,0.5])\n",
        "                if s.position==3:\n",
        "                    r=self.rewards_matrix[1][0] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[0][0] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([0,3],p=[0.5,0.5])\n",
        "                if s.position==0:\n",
        "                    r=self.rewards_matrix[0][0] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[1][0] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "        elif(s.position==7):\n",
        "            if(action==0):\n",
        "                s.position=np.random.choice([6,8],p=[0.5,0.5])\n",
        "                if(s.position==6):\n",
        "                    r=self.rewards_matrix[2][0] -0.1\n",
        "                    s.term=False\n",
        "                else:\n",
        "                    r=self.rewards_matrix[2][2] \n",
        "                    s.term=True\n",
        "                \n",
        "            elif (action==1):\n",
        "                s.position=np.random.choice([6,8],p=[0.5,0.5])\n",
        "                if(s.position==8):\n",
        "                    r=self.rewards_matrix[2][1] \n",
        "                    s.term=True\n",
        "                else:\n",
        "                    r=self.rewards_matrix[2][2] -0.2\n",
        "                    s.term=False\n",
        "                \n",
        "                \n",
        "            elif(action==2):\n",
        "                s.position=np.random.choice([1,4],p=[0.5,0.5])\n",
        "                if s.position==4:\n",
        "                    r=self.rewards_matrix[1][1] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[0][1] -0.2\n",
        "                s.term=False\n",
        "                \n",
        "            elif(action==3):\n",
        "                s.position=np.random.choice([1,4],p=[0.5,0.5])\n",
        "                if s.position==0:\n",
        "                    r=self.rewards_matrix[0][1] -0.1\n",
        "                else :\n",
        "                    r=self.rewards_matrix[1][1] -0.2\n",
        "                s.term=False\n",
        "        \n",
        "        # Note that, we did not include state - 5, 8 since they are termination states.\n",
        "        \n",
        "        return s, r\n",
        "                \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNPVKX2MOyj0"
      },
      "source": [
        "### Moves by Human Player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeFeN6T6Oyj2"
      },
      "source": [
        "env = Environment()\n",
        "s = env.get_start_state()\n",
        "total_r = 0\n",
        "while True:\n",
        "    action = int(input(\"Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->\"))\n",
        "    s,r = env.step(s,action)\n",
        "    total_r += r;\n",
        "    print(\"Your new state is \", s.position)\n",
        "    print(\"Your current reward is\", r )\n",
        "    print(\"Your Total reward is\", total_r )\n",
        "    if (s.term):\n",
        "        print(\"You entered the termination\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmOvtVtqOyj6"
      },
      "source": [
        "### Monte-Carlo Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69agITjKOyj7"
      },
      "source": [
        "class MC_Agent:\n",
        "    \n",
        "    def __init__(self, environment, n0):\n",
        "        \n",
        "        self.max_steps = 10000\n",
        "        \n",
        "        self.n0 = float(n0)\n",
        "        self.env = environment\n",
        "        \n",
        "        # N(s) is the number of times that state s has been visited\n",
        "        # N(s,a) is the number of times that action a has been selected from state s.\n",
        "        self.N = np.zeros((self.env.observation_space,\n",
        "                           self.env.action_space))\n",
        "        \n",
        "        self.Q = np.zeros((self.env.observation_space,\n",
        "                           self.env.action_space))\n",
        "\n",
        "        # Initialise the value function to zero. \n",
        "        self.V = np.zeros(self.env.observation_space)\n",
        "        \n",
        "        # self.count_wins = 0\n",
        "        self.iterations = 0\n",
        "\n",
        "\n",
        "    \n",
        "    # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
        "    # ε-greedy exploration strategy with εt = N0/(N0 + N(st))\n",
        "    def get_action(self, state):\n",
        "        \n",
        "        # state.position is just a number lying between 0-8.\n",
        "        n_visits = sum(self.N[state.position, :])\n",
        "\n",
        "        # epsilon = N0/(N0 + N(st))\n",
        "        # # if (n_visits == self.max_steps/2):\n",
        "        # #   curr_epsilon = 0.5\n",
        "        # # else:\n",
        "        curr_epsilon = N0/(N0 + n_visits)\n",
        "\n",
        "        # epsilon greedy policy\n",
        "        if random.random() < curr_epsilon:\n",
        "            r_action = np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
        "            return r_action\n",
        "        else:\n",
        "            action =  np.argmax(self.Q[state.position, :])\n",
        "            return action\n",
        "\n",
        "        \n",
        "        \n",
        "    def train(self, iterations):        \n",
        "        \n",
        "        # Loop episodes\n",
        "        for episode in range(iterations):\n",
        "            \n",
        "            curr_step = 0\n",
        "            \n",
        "            episode_pairs = []\n",
        "            \n",
        "            # get initial state for current episode\n",
        "            s = self.env.get_start_state()\n",
        "            \n",
        "            tot_r = 0\n",
        "            \n",
        "            # Execute until game ends\n",
        "            while not s.term:\n",
        "                \n",
        "                curr_step += 1\n",
        "                if (curr_step > self.max_steps):\n",
        "                    break\n",
        "                \n",
        "                # get action with epsilon greedy policy\n",
        "                a = self.get_action(s)\n",
        "               \n",
        "                # store action state pairs\n",
        "                episode_pairs.append((s,a))\n",
        "                \n",
        "                # update visits\n",
        "                # N(s,a) is the number of times that action a has been selected from state s. \n",
        "                self.N[s.position, a] += 1\n",
        "                \n",
        "                # execute action\n",
        "                s,r = self.env.step(s, a)\n",
        "                \n",
        "                tot_r += r\n",
        "\n",
        "\n",
        "            # Update Action value function accordingly\n",
        "            for curr_s, curr_a in episode_pairs:\n",
        "                pos_idx = curr_s.position\n",
        "                action_idx = curr_a\n",
        "        \n",
        "                step = 1.0 / self.N[pos_idx, action_idx]   # Alpha\n",
        "            \n",
        "                error = tot_r - self.Q[pos_idx, action_idx]\n",
        "                \n",
        "                self.Q[pos_idx,action_idx] += step * error\n",
        "                \n",
        "        return (self.Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XMFAfRXOyj8"
      },
      "source": [
        "N0 = 100\n",
        "agent = MC_Agent(Environment(), N0)\n",
        "Q = agent.train(50000)\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1obMRi1Oyj-"
      },
      "source": [
        "### Moves by RL Agent ( MC )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3GvDIaQOyj_"
      },
      "source": [
        "env = Environment()\n",
        "s = env.get_start_state()\n",
        "total_r = 0\n",
        "while True:\n",
        "    action = np.argmax(Q[s.position,:])\n",
        "    s,r = env.step(s,action)\n",
        "    total_r += r;\n",
        "    print(\"Your new state is \", s.position)\n",
        "    print(\"Your current reward is\", r )\n",
        "    print(\"Your Total reward is\", total_r )\n",
        "    if (s.term):\n",
        "        print(\"You entered the termination\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNVqsQ58OykA"
      },
      "source": [
        "### TD Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99VrSp_fOykB"
      },
      "source": [
        "class Sarsa_Agent:\n",
        "    \n",
        "    def __init__(self, environment, n0, mlambda):\n",
        "        \n",
        "        self.n0 = float(n0)\n",
        "        self.env = environment\n",
        "        self.mlambda = mlambda\n",
        "        \n",
        "        # N(s) is the number of times that state s has been visited\n",
        "        # N(s,a) is the number of times that action a has been selected from state s.\n",
        "        self.N = np.zeros((self.env.observation_space,\n",
        "                           self.env.action_space))\n",
        "        \n",
        "        self.Q = np.zeros((self.env.observation_space,\n",
        "                           self.env.action_space))\n",
        "        \n",
        "        self.E =np.zeros((self.env.observation_space,\n",
        "                           self.env.action_space))\n",
        "\n",
        "        # Initialise the value function to zero. \n",
        "        self.V = np.zeros(self.env.observation_space)\n",
        "        \n",
        "        self.count_wins = 0\n",
        "        self.iterations = 0\n",
        "\n",
        "    # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
        "    # ε-greedy exploration strategy with εt = N0/(N0 + N(st)), \n",
        "    def train_get_action(self, state):\n",
        "\n",
        "        try:\n",
        "            n_visits = sum(self.N[state.position, :])\n",
        "        except:\n",
        "            n_visits = 0        \n",
        "\n",
        "        # epsilon = N0/(N0 + N(st)\n",
        "        curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
        "        \n",
        "        # epsilon greedy policy\n",
        "        if random.random() < curr_epsilon:\n",
        "            r_action = np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
        "            return r_action\n",
        "        else:\n",
        "            action =  np.argmax(self.Q[state.position, :]) \n",
        "            return action\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        action = np.argmax(self.Q[state.position, :]) \n",
        "        return action\n",
        "\n",
        "\n",
        "    def train(self, iterations):        \n",
        "        \n",
        "        # Loop episodes\n",
        "        for episode in range(iterations):\n",
        "            \n",
        "            self.E =np.zeros((self.env.observation_space, self.env.action_space))\n",
        "                \n",
        "            # get initial state for current episode\n",
        "            s = self.env.get_start_state()\n",
        "            a = self.train_get_action(s)\n",
        "            a_next = a\n",
        "            \n",
        "            # Execute until game ends\n",
        "            while not s.term:\n",
        "                # update visits\n",
        "                \n",
        "                self.N[s.position, a] += 1\n",
        "                \n",
        "                # execute action\n",
        "                s_next, r = self.env.step(s, a)\n",
        "                \n",
        "                q = self.Q[s.position, a]\n",
        "                                \n",
        "                if not s_next.term:\n",
        "                    # choose next action with epsilon greedy policy\n",
        "                    a_next = self.train_get_action(s_next)\n",
        "                    \n",
        "                    next_q = self.Q[s.position, a_next]\n",
        "                    delta = r + next_q - q\n",
        "                else:\n",
        "                    delta = r - q\n",
        "\n",
        "                \n",
        "                self.E[s.position, a] += 1\n",
        "                alpha = 1.0  / (self.N[s.position, a])\n",
        "                \n",
        "                update = alpha * delta * self.E\n",
        "                self.Q += update\n",
        "                self.E *= self.mlambda\n",
        "\n",
        "                # reassign s and a\n",
        "                s = s_next\n",
        "                a = a_next\n",
        "    \n",
        "        return (self.Q)\n",
        "\n",
        "\n",
        "      \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOK6chmuOykC"
      },
      "source": [
        "N0 = 100\n",
        "agent = Sarsa_Agent(Environment(), N0, 0.9)\n",
        "Q = agent.train(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfDEfFmtOykC"
      },
      "source": [
        "### Moves by RL Agent ( TD )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bsb6O3FOykD"
      },
      "source": [
        "env = Environment()\n",
        "s = env.get_start_state()\n",
        "total_r = 0\n",
        "while True:\n",
        "    action = np.argmax(Q[s.position,:])\n",
        "    s,r = env.step(s,action)\n",
        "    total_r += r\n",
        "    print(\"Your new state is \", s.position)\n",
        "    print(\"Your current reward is\", r )\n",
        "    print(\"Your Total reward is\", total_r )\n",
        "    if (s.term):\n",
        "        print(\"You entered the termination\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "For implementation of the Algorithms, The following [repo] (https://github.com/analog-rl/Easy21) is referred in addition to the Lecture Slides"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}