{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgF-H_HTGu8s"
   },
   "source": [
    "#### Reinforcement Learning Agent to play **Frozen Lake** Game!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5O43Y6jHaMU"
   },
   "source": [
    "**Game Rules:**\n",
    "- We are in a 3x3 grid world which is 0-indexed.\n",
    "- Starting from (0,0), Player should move in the grid inorder to maximise the reward.\n",
    "- The player will receive a reward of +1 if he enters the grid numbered with 4/6 ( Treasure ).\n",
    "- The player will receive a reward of -10 & the game terminates if enters the grid numbered 5  ( Danger! ).\n",
    "- The player will receive a reward of +10 & the game terminates if enters the grid numbered 8 ( End Point ). \n",
    "- In each step, the player receives a reward of -0.1. This makes sure that RL agent finds the best shortest path.\n",
    "-  In each step, player can move in any one of the 4 directions: Up, Down, Left, Right.\n",
    "- The randomness in environment is as follows:\n",
    " **Once, the player decides an action ( direction ) , with 0.5 probability player will take one step in the desired direction and  with 0.5 player slip and takes 2 steps in that direction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqqce26FN0wH"
   },
   "source": [
    "### Basic Game Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "s9kwb2D3Gz8D",
    "outputId": "18ad4f12-ef75-4876-b84a-1cf7ea1ef814"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Importing all the required libraries.\n",
    "import random\n",
    "import numpy as np\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fx4cDhwIK1sl"
   },
   "outputs": [],
   "source": [
    "# Defining a class for all possible actions taken by the Player.\n",
    "class Action:\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.L=0\n",
    "        self.R=1\n",
    "        self.U=2\n",
    "        self.D=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ujYLh4rMGdSV"
   },
   "outputs": [],
   "source": [
    "# Defining a class for simulating the randomness of the environment & its impact on actions of the player!\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.action_space=4\n",
    "        self.observation_space=9\n",
    "        self.state=1\n",
    "        self.done=False\n",
    "        self.reward=[[0,0,0],[0,1,-10],[1,0,10]]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state=0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self,action):\n",
    "\n",
    "        self.Reward=0\n",
    "        Act = Action()\n",
    "\n",
    "         # Player is in the 1st cell        \n",
    "        if(self.state==0):\n",
    "\n",
    "              # Choosen action is to move LEFT.\n",
    "            if(action==Act.L):\n",
    "\n",
    "                # With prob 0.5 each he will move to state 1 or 2 due to randomness of environment.\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "\n",
    "                # If he moves to state 2, He took 1 step, so in rewards, we subtract 0.1\n",
    "                if ( self.state==2 ):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "\n",
    "                # If he moves to state 1, He took 2 steps, so in rewards, we subtract 0.2\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                \n",
    "                # Note: Subtracting 0.1 helps in finding shortest path.\n",
    "\n",
    "                self.done=False\n",
    "                \n",
    "            \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==1):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==2):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==1):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==0):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "        elif(self.state==3):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==4):\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                   \n",
    "        elif(self.state==4):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==3):\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==6):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==7):\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==7):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==6):\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        return self.state,self.Reward,self.done\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNPVKX2MOyj0"
   },
   "source": [
    "### Moves by Human Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeFeN6T6Oyj2",
    "outputId": "4725493f-cd86-4e6c-d27a-7606a9889280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->2\n",
      "You took Downward turn ↓ \n",
      "Your new state is  6\n",
      "Your current reward is 0.9\n",
      "Your Total reward is 0.9\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "You took Right turn → \n",
      "Your new state is  7\n",
      "Your current reward is -0.1\n",
      "Your Total reward is 0.8\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "You took Right turn → \n",
      "Your new state is  6\n",
      "Your current reward is 0.8\n",
      "Your Total reward is 1.6\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->3\n",
      "You took Upward turn ↑ \n",
      "Your new state is  3\n",
      "Your current reward is -0.2\n",
      "Your Total reward is 1.4000000000000001\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "You took Right turn → \n",
      "Your new state is  5\n",
      "Your current reward is -10\n",
      "Your Total reward is -8.6\n",
      "You entered the termination. Bye!\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_r = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    action = int(input(\"Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->\"))\n",
    "\n",
    "    if (action==0):\n",
    "        print(\"You took Left turn ← \")\n",
    "        \n",
    "    elif(action==1):\n",
    "        print(\"You took Right turn → \")\n",
    "        \n",
    "    elif(action==3):\n",
    "        print(\"You took Upward turn ↑ \")\n",
    "        \n",
    "    else:\n",
    "        print(\"You took Downward turn ↓ \")\n",
    "\n",
    "    new_state, reward, done = env.step(action)\n",
    "\n",
    "    total_r += reward;\n",
    "\n",
    "    print(\"Your new state is \", new_state)\n",
    "    print(\"Your current reward is\", reward )\n",
    "    print(\"Your Total reward is\", total_r )\n",
    "\n",
    "    if (done):\n",
    "        print(\"You entered the termination. Bye!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKgxtwWVOABz"
   },
   "source": [
    "### Training RL agent using **Monte Carlo Control**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FoAFHmsdGlIl"
   },
   "outputs": [],
   "source": [
    "class MC_agent:\n",
    "\n",
    "    def __init__(self,enviroment):\n",
    "        \n",
    "        self.num_episodes = 1000\n",
    "        self.steps_per_episode = 500\n",
    "        self.env = enviroment\n",
    "        self.episode_rewards = []\n",
    "\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "        \n",
    "        # Q-table for Q(s,a) and Initialise the value function to zero. \n",
    "        self.Q = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "\n",
    "\n",
    "    # get optimal action, with epsilon exploration using ε-greedy exploration strategy\n",
    "    # with εt = 1/K where K is the Epsiode Number\n",
    "    def get_action(self, state, epsiode):\n",
    "\n",
    "        curr_epsilon = 1/(epsiode)\n",
    "\n",
    "        # epsilon greedy policy\n",
    "        if np.random.uniform(0, 1) < curr_epsilon:\n",
    "            r_action = np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
    "            return r_action\n",
    "        else:\n",
    "            action =  np.argmax(self.Q[state, :])\n",
    "            return action\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        # Loop episodes\n",
    "        for episode in range(1,self.num_episodes+1):\n",
    "\n",
    "            # for storing each state, action pair in each step of episodes\n",
    "            history = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            rewards_current_episode = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            for step in range(self.steps_per_episode):\n",
    "\n",
    "                action = MC_agent.get_action(self,state, episode)\n",
    "               \n",
    "                # store action state pairs\n",
    "                history.append((state,action))\n",
    "                \n",
    "                # update visits\n",
    "                # N(s,a) is the number of times that action a has been selected from state s. \n",
    "                self.N[state, action] += 1\n",
    "                \n",
    "                # execute action\n",
    "                state,reward,done = self.env.step(action)\n",
    "                \n",
    "                rewards_current_episode += reward\n",
    "                \n",
    "                # When Termination state is reached\n",
    "                if(done == True):\n",
    "                    break\n",
    "             \n",
    "            self.episode_rewards.append(rewards_current_episode)\n",
    "\n",
    "            # Update Action value function accordingly\n",
    "            for curr_state, curr_action in history:\n",
    "                \n",
    "                # Alpha(learning rate)\n",
    "                step = 1.0 / self.N[curr_state, curr_action]   \n",
    "            \n",
    "                error = rewards_current_episode - self.Q[curr_state, curr_action]\n",
    "                \n",
    "                self.Q[curr_state, curr_action] += step * error\n",
    "                \n",
    "    def ShowActionValueFunc(self):\n",
    "        \n",
    "        print(\"\\n <-- Q(s,a) in Q-Table Format --> \\n\")\n",
    "        print(self.Q)\n",
    "\n",
    "        # Saving Q table\n",
    "        np.save('Q_table_MC',self.Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i868Bh_HGlOC",
    "outputId": "c4645849-faeb-4396-a598-cfafe00a37df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <-- Q(s,a) in Q-Table Format --> \n",
      "\n",
      "[[ 78.73662132  82.68464661  72.80275221  91.30770945]\n",
      " [ 95.84514047  91.83810813 138.53331373  51.93213272]\n",
      " [ 42.525       78.68186506  -4.2          3.8       ]\n",
      " [ 62.97681267  68.67493173  82.84229395  91.34397048]\n",
      " [ 70.33991224  74.26241993 138.46304624  61.39756979]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 80.40656579  89.75822416  73.09673854  91.66867301]\n",
      " [ 83.0732932   85.21336562 106.46771132 138.52376344]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "agent = MC_agent(Environment())\n",
    "agent.train()\n",
    "agent.ShowActionValueFunc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1obMRi1Oyj-"
   },
   "source": [
    "### Moves by RL Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j3GvDIaQOyj_"
   },
   "outputs": [],
   "source": [
    "# Load the Q-Values from the saved table.\n",
    "q_table  = np.load('Q_table_MC.npy')\n",
    "\n",
    "env = Environment()\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_r = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    action = np.argmax(q_table[state,:])\n",
    "\n",
    "    if (action==0):\n",
    "        print(\"You took Left turn ← \")\n",
    "        \n",
    "    elif(action==1):\n",
    "        print(\"You took Right turn → \")\n",
    "        \n",
    "    elif(action==3):\n",
    "        print(\"You took Upward turn ↑ \")\n",
    "        \n",
    "    else:\n",
    "        print(\"You took Downward turn ↓ \")\n",
    "\n",
    "    new_state, reward, done = env.step(action)\n",
    "\n",
    "    total_r += reward;\n",
    "\n",
    "    print(\"Your new state is \", new_state)\n",
    "    print(\"Your current reward is\", reward )\n",
    "    print(\"Your Total reward is\", total_r )\n",
    "\n",
    "    if (done):\n",
    "        print(\"You entered the termination. Bye!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnUXqA3sVSX3"
   },
   "source": [
    "### Interpretation:\n",
    "\n",
    "**The divergence of Total reward when game is played by the RL agent trained under Monte Carlo suggests that our RL agent is working pretty much well!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h1obMRi1Oyj-"
   ],
   "name": "RL_agent_MC_FrozenLake_game.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
